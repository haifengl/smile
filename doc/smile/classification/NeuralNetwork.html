<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (version 1.7.0_75) on Wed Apr 15 10:40:20 EDT 2015 -->
<title>NeuralNetwork</title>
<meta name="date" content="2015-04-15">
<link rel="stylesheet" type="text/css" href="../../stylesheet.css" title="Style">
</head>
<body>
<script type="text/javascript"><!--
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="NeuralNetwork";
    }
//-->
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar_top">
<!--   -->
</a><a href="#skip-navbar_top" title="Skip navigation links"></a><a name="navbar_top_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NeuralNetwork.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../index-files/index-1.html">Index</a></li>
<li><a href="../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../smile/classification/NaiveBayes.Trainer.html" title="class in smile.classification"><span class="strong">Prev Class</span></a></li>
<li><a href="../../smile/classification/NeuralNetwork.ActivationFunction.html" title="enum in smile.classification"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../index.html?smile/classification/NeuralNetwork.html" target="_top">Frames</a></li>
<li><a href="NeuralNetwork.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li><a href="#nested_class_summary">Nested</a>&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">smile.classification</div>
<h2 title="Class NeuralNetwork" class="title">Class NeuralNetwork</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>java.lang.Object</li>
<li>
<ul class="inheritance">
<li>smile.classification.NeuralNetwork</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<dl>
<dt>All Implemented Interfaces:</dt>
<dd><a href="../../smile/classification/Classifier.html" title="interface in smile.classification">Classifier</a>&lt;double[]&gt;, <a href="../../smile/classification/OnlineClassifier.html" title="interface in smile.classification">OnlineClassifier</a>&lt;double[]&gt;</dd>
</dl>
<hr>
<br>
<pre>public class <span class="strong">NeuralNetwork</span>
extends java.lang.Object
implements <a href="../../smile/classification/OnlineClassifier.html" title="interface in smile.classification">OnlineClassifier</a>&lt;double[]&gt;</pre>
<div class="block">Multilayer perceptron neural network. 
 An MLP consists of several layers of nodes, interconnected through weighted
 acyclic arcs from each preceding layer to the following, without lateral or
 feedback connections. Each node calculates a transformed weighted linear
 combination of its inputs (output activations from the preceding layer), with
 one of the weights acting as a trainable bias connected to a constant input.
 The transformation, called activation function, is a bounded non-decreasing
 (non-linear) function, such as the sigmoid functions (ranges from 0 to 1).
 Another popular activation function is hyperbolic tangent which is actually
 equivalent to the sigmoid function in shape but ranges from -1 to 1. 
 More specialized activation functions include radial basis functions which
 are used in RBF networks.
 <p>
 The representational capabilities of a MLP are determined by the range of
 mappings it may implement through weight variation. Single layer perceptrons
 are capable of solving only linearly separable problems. With the sigmoid
 function as activation function, the single-layer network is identical
 to the logistic regression model.
 <p>
 The universal approximation theorem for neural networks states that every
 continuous function that maps intervals of real numbers to some output
 interval of real numbers can be approximated arbitrarily closely by a
 multi-layer perceptron with just one hidden layer. This result holds only
 for restricted classes of activation functions, which are extremely complex
 and NOT smooth for subtle mathematical reasons. On the other hand, smoothness
 is important for gradient descent learning. Besides, the proof is not
 constructive regarding the number of neurons required or the settings of
 the weights. Therefore, complex systems will have more layers of neurons
 with some having increased layers of input neurons and output neurons
 in practice.
 <p>
 The most popular algorithm to train MLPs is back-propagation, which is a
 gradient descent method. Based on chain rule, the algorithm propagates the
 error back through the network and adjusts the weights of each connection in
 order to reduce the value of the error function by some small amount.
 For this reason, back-propagation can only be applied on networks with
 differentiable activation functions.
 <p>
 During error back propagation, we usually times the gradient with a small
 number &eta;, called learning rate, which is carefully selected to ensure
 that the network converges to a local minimum of the error function
 fast enough, without producing oscillations. One way to avoid oscillation 
 at large &eta;, is to make the change in weight dependent on the past weight
 change by adding a momentum term.
 <p>
 Although the back-propagation algorithm may performs gradient
 descent on the total error of all instances in a batch way, 
 the learning rule is often applied to each instance separately in an online
 way or stochastic way. There exists empirical indication that the stochastic
 way results in faster convergence.
 <p>
 In practice, the problem of over-fitting has emerged. This arises in
 convoluted or over-specified systems when the capacity of the network
 significantly exceeds the needed free parameters. There are two general
 approaches for avoiding this problem: The first is to use cross-validation
 and similar techniques to check for the presence of over-fitting and
 optimally select hyper-parameters such as to minimize the generalization
 error. The second is to use some form of regularization, which emerges
 naturally in a Bayesian framework, where the regularization can be
 performed by selecting a larger prior probability over simpler models;
 but also in statistical learning theory, where the goal is to minimize over
 the "empirical risk" and the "structural risk".
 <p>
 For neural networks, the input patterns usually should be scaled/standardized.
 Commonly, each input variable is scaled into interval [0, 1] or to have
 mean 0 and standard deviation 1.
 <p>
 For penalty functions and output units, the following natural pairings are
 recommended:
 <ul>
 <li> linear output units and a least squares penalty function.
 <li> a two-class cross-entropy penalty function and a logistic
 activation function.
 <li> a multi-class cross-entropy penalty function and a softmax
 activation function.
 </ul>
 By assigning a softmax activation function on the output layer of
 the neural network for categorical target variables, the outputs
 can be interpreted as posterior probabilities, which are very useful.</div>
<dl><dt><span class="strong">Author:</span></dt>
  <dd>Haifeng Li</dd></dl>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== NESTED CLASS SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="nested_class_summary">
<!--   -->
</a>
<h3>Nested Class Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Nested Class Summary table, listing nested classes, and an explanation">
<caption><span>Nested Classes</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Class and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static class&nbsp;</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.ActivationFunction.html" title="enum in smile.classification">NeuralNetwork.ActivationFunction</a></strong></code>
<div class="block">The types of activation functions in output layer.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>static class&nbsp;</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.ErrorFunction.html" title="enum in smile.classification">NeuralNetwork.ErrorFunction</a></strong></code>
<div class="block">The types of error functions.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>static class&nbsp;</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.Trainer.html" title="class in smile.classification">NeuralNetwork.Trainer</a></strong></code>
<div class="block">Trainer for neural networks.</div>
</td>
</tr>
</table>
</li>
</ul>
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><strong><a href="../../smile/classification/NeuralNetwork.html#NeuralNetwork(smile.classification.NeuralNetwork.ErrorFunction,%20int...)">NeuralNetwork</a></strong>(<a href="../../smile/classification/NeuralNetwork.ErrorFunction.html" title="enum in smile.classification">NeuralNetwork.ErrorFunction</a>&nbsp;error,
             int...&nbsp;numUnits)</code>
<div class="block">Constructor.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colOne"><code><strong><a href="../../smile/classification/NeuralNetwork.html#NeuralNetwork(smile.classification.NeuralNetwork.ErrorFunction,%20smile.classification.NeuralNetwork.ActivationFunction,%20int...)">NeuralNetwork</a></strong>(<a href="../../smile/classification/NeuralNetwork.ErrorFunction.html" title="enum in smile.classification">NeuralNetwork.ErrorFunction</a>&nbsp;error,
             <a href="../../smile/classification/NeuralNetwork.ActivationFunction.html" title="enum in smile.classification">NeuralNetwork.ActivationFunction</a>&nbsp;activation,
             int...&nbsp;numUnits)</code>
<div class="block">Constructor.</div>
</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method_summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="overviewSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span>Methods</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr class="altColor">
<td class="colFirst"><code><a href="../../smile/classification/NeuralNetwork.html" title="class in smile.classification">NeuralNetwork</a></code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#clone()">clone</a></strong>()</code>&nbsp;</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#getLearningRate()">getLearningRate</a></strong>()</code>
<div class="block">Returns the learning rate.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#getMomentum()">getMomentum</a></strong>()</code>
<div class="block">Returns the momentum factor.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#getWeightDecay()">getWeightDecay</a></strong>()</code>
<div class="block">Returns the weight decay factor.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#learn(double[][],%20int[])">learn</a></strong>(double[][]&nbsp;x,
     int[]&nbsp;y)</code>
<div class="block">Trains the neural network with the given dataset for one epoch by
 stochastic gradient descent.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#learn(double[],%20double[],%20double)">learn</a></strong>(double[]&nbsp;x,
     double[]&nbsp;y,
     double&nbsp;weight)</code>
<div class="block">Update the neural network with given instance and associated target value.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#learn(double[],%20int)">learn</a></strong>(double[]&nbsp;x,
     int&nbsp;y)</code>
<div class="block">Online update the classifier with a new training instance.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#learn(double[],%20int,%20double)">learn</a></strong>(double[]&nbsp;x,
     int&nbsp;y,
     double&nbsp;weight)</code>
<div class="block">Online update the neural network with a new training instance.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>int</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#predict(double[])">predict</a></strong>(double[]&nbsp;x)</code>
<div class="block">Predict the class of a given instance.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>int</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#predict(double[],%20double[])">predict</a></strong>(double[]&nbsp;x,
       double[]&nbsp;y)</code>
<div class="block">Predict the target value of a given instance.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#setLearningRate(double)">setLearningRate</a></strong>(double&nbsp;eta)</code>
<div class="block">Sets the learning rate.</div>
</td>
</tr>
<tr class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#setMomentum(double)">setMomentum</a></strong>(double&nbsp;alpha)</code>
<div class="block">Sets the momentum factor.</div>
</td>
</tr>
<tr class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><strong><a href="../../smile/classification/NeuralNetwork.html#setWeightDecay(double)">setWeightDecay</a></strong>(double&nbsp;lambda)</code>
<div class="block">Sets the weight decay factor.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods_inherited_from_class_java.lang.Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;java.lang.Object</h3>
<code>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor_detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="NeuralNetwork(smile.classification.NeuralNetwork.ErrorFunction, int...)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>NeuralNetwork</h4>
<pre>public&nbsp;NeuralNetwork(<a href="../../smile/classification/NeuralNetwork.ErrorFunction.html" title="enum in smile.classification">NeuralNetwork.ErrorFunction</a>&nbsp;error,
             int...&nbsp;numUnits)</pre>
<div class="block">Constructor. The activation function of output layer will be chosen
 by natural pairing based on the error function and the number of
 classes.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>error</code> - the error function.</dd><dd><code>numUnits</code> - the number of units in each layer.</dd></dl>
</li>
</ul>
<a name="NeuralNetwork(smile.classification.NeuralNetwork.ErrorFunction, smile.classification.NeuralNetwork.ActivationFunction, int...)">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>NeuralNetwork</h4>
<pre>public&nbsp;NeuralNetwork(<a href="../../smile/classification/NeuralNetwork.ErrorFunction.html" title="enum in smile.classification">NeuralNetwork.ErrorFunction</a>&nbsp;error,
             <a href="../../smile/classification/NeuralNetwork.ActivationFunction.html" title="enum in smile.classification">NeuralNetwork.ActivationFunction</a>&nbsp;activation,
             int...&nbsp;numUnits)</pre>
<div class="block">Constructor.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>error</code> - the error function.</dd><dd><code>activation</code> - the activation function of output layer.</dd><dd><code>numUnits</code> - the number of units in each layer.</dd></dl>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method_detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="clone()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>clone</h4>
<pre>public&nbsp;<a href="../../smile/classification/NeuralNetwork.html" title="class in smile.classification">NeuralNetwork</a>&nbsp;clone()</pre>
<dl>
<dt><strong>Overrides:</strong></dt>
<dd><code>clone</code>&nbsp;in class&nbsp;<code>java.lang.Object</code></dd>
</dl>
</li>
</ul>
<a name="setLearningRate(double)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setLearningRate</h4>
<pre>public&nbsp;void&nbsp;setLearningRate(double&nbsp;eta)</pre>
<div class="block">Sets the learning rate.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>eta</code> - the learning rate.</dd></dl>
</li>
</ul>
<a name="getLearningRate()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getLearningRate</h4>
<pre>public&nbsp;double&nbsp;getLearningRate()</pre>
<div class="block">Returns the learning rate.</div>
</li>
</ul>
<a name="setMomentum(double)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setMomentum</h4>
<pre>public&nbsp;void&nbsp;setMomentum(double&nbsp;alpha)</pre>
<div class="block">Sets the momentum factor.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>alpha</code> - the momentum factor.</dd></dl>
</li>
</ul>
<a name="getMomentum()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getMomentum</h4>
<pre>public&nbsp;double&nbsp;getMomentum()</pre>
<div class="block">Returns the momentum factor.</div>
</li>
</ul>
<a name="setWeightDecay(double)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setWeightDecay</h4>
<pre>public&nbsp;void&nbsp;setWeightDecay(double&nbsp;lambda)</pre>
<div class="block">Sets the weight decay factor. After each weight update, every weight
 is simply ''decayed'' or shrunk according w = w * (1 - eta * lambda).</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>lambda</code> - the weight decay for regularization.</dd></dl>
</li>
</ul>
<a name="getWeightDecay()">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getWeightDecay</h4>
<pre>public&nbsp;double&nbsp;getWeightDecay()</pre>
<div class="block">Returns the weight decay factor.</div>
</li>
</ul>
<a name="predict(double[], double[])">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>predict</h4>
<pre>public&nbsp;int&nbsp;predict(double[]&nbsp;x,
          double[]&nbsp;y)</pre>
<div class="block">Predict the target value of a given instance. Note that this method is NOT
 multi-thread safe.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../smile/classification/Classifier.html#predict(T,%20double[])">predict</a></code>&nbsp;in interface&nbsp;<code><a href="../../smile/classification/Classifier.html" title="interface in smile.classification">Classifier</a>&lt;double[]&gt;</code></dd>
<dt><span class="strong">Parameters:</span></dt><dd><code>x</code> - the instance.</dd><dd><code>y</code> - the array to store network output on output. For softmax
 activation function, these are estimated posteriori probabilities.</dd>
<dt><span class="strong">Returns:</span></dt><dd>the predicted class label.</dd></dl>
</li>
</ul>
<a name="predict(double[])">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>predict</h4>
<pre>public&nbsp;int&nbsp;predict(double[]&nbsp;x)</pre>
<div class="block">Predict the class of a given instance. Note that this method is NOT
 multi-thread safe.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../smile/classification/Classifier.html#predict(T)">predict</a></code>&nbsp;in interface&nbsp;<code><a href="../../smile/classification/Classifier.html" title="interface in smile.classification">Classifier</a>&lt;double[]&gt;</code></dd>
<dt><span class="strong">Parameters:</span></dt><dd><code>x</code> - the instance.</dd>
<dt><span class="strong">Returns:</span></dt><dd>the predicted class label.</dd></dl>
</li>
</ul>
<a name="learn(double[], double[], double)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>learn</h4>
<pre>public&nbsp;double&nbsp;learn(double[]&nbsp;x,
           double[]&nbsp;y,
           double&nbsp;weight)</pre>
<div class="block">Update the neural network with given instance and associated target value.
 Note that this method is NOT multi-thread safe.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>x</code> - the training instance.</dd><dd><code>y</code> - the target value.</dd><dd><code>weight</code> - a positive weight value associated with the training instance.</dd>
<dt><span class="strong">Returns:</span></dt><dd>the weighted training error before back-propagation.</dd></dl>
</li>
</ul>
<a name="learn(double[], int)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>learn</h4>
<pre>public&nbsp;void&nbsp;learn(double[]&nbsp;x,
         int&nbsp;y)</pre>
<div class="block"><strong>Description copied from interface:&nbsp;<code><a href="../../smile/classification/OnlineClassifier.html#learn(T,%20int)">OnlineClassifier</a></code></strong></div>
<div class="block">Online update the classifier with a new training instance.
 In general, this method may be NOT multi-thread safe.</div>
<dl>
<dt><strong>Specified by:</strong></dt>
<dd><code><a href="../../smile/classification/OnlineClassifier.html#learn(T,%20int)">learn</a></code>&nbsp;in interface&nbsp;<code><a href="../../smile/classification/OnlineClassifier.html" title="interface in smile.classification">OnlineClassifier</a>&lt;double[]&gt;</code></dd>
<dt><span class="strong">Parameters:</span></dt><dd><code>x</code> - training instance.</dd><dd><code>y</code> - training label.</dd></dl>
</li>
</ul>
<a name="learn(double[], int, double)">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>learn</h4>
<pre>public&nbsp;void&nbsp;learn(double[]&nbsp;x,
         int&nbsp;y,
         double&nbsp;weight)</pre>
<div class="block">Online update the neural network with a new training instance.
 Note that this method is NOT multi-thread safe.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>x</code> - training instance.</dd><dd><code>y</code> - training label.</dd><dd><code>weight</code> - a positive weight value associated with the training instance.</dd></dl>
</li>
</ul>
<a name="learn(double[][], int[])">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>learn</h4>
<pre>public&nbsp;void&nbsp;learn(double[][]&nbsp;x,
         int[]&nbsp;y)</pre>
<div class="block">Trains the neural network with the given dataset for one epoch by
 stochastic gradient descent.</div>
<dl><dt><span class="strong">Parameters:</span></dt><dd><code>x</code> - training instances.</dd><dd><code>y</code> - training labels in [0, k), where k is the number of classes.</dd></dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar_bottom">
<!--   -->
</a><a href="#skip-navbar_bottom" title="Skip navigation links"></a><a name="navbar_bottom_firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NeuralNetwork.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../index-files/index-1.html">Index</a></li>
<li><a href="../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../smile/classification/NaiveBayes.Trainer.html" title="class in smile.classification"><span class="strong">Prev Class</span></a></li>
<li><a href="../../smile/classification/NeuralNetwork.ActivationFunction.html" title="enum in smile.classification"><span class="strong">Next Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../index.html?smile/classification/NeuralNetwork.html" target="_top">Frames</a></li>
<li><a href="NeuralNetwork.html" target="_top">No Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../allclasses-noframe.html">All Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li><a href="#nested_class_summary">Nested</a>&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor_detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method_detail">Method</a></li>
</ul>
</div>
<a name="skip-navbar_bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
