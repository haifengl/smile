<!-- prettify js and CSS -->
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?lang=scala"></script>

<!-- scroll/follow sidebar -->
<script src="js/follow-sidebar.js" type="text/javascript"></script>

<div class="col-md-3 col-md-push-9 hidden-xs hidden-sm">
    <div id="sidebar">
        <div class="sidebar-toc" style="margin-bottom: 20px;">
            <p class="toc-header">Contents</p>
            <div id="toc"></div>
        </div>
        <div id="search">
            <script>
                (function() {
                    var cx = '010264411143030149390:ajvee_ckdzs';
                    var gcse = document.createElement('script');
                    gcse.type = 'text/javascript';
                    gcse.async = true;
                    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
                            '//cse.google.com/cse.js?cx=' + cx;
                    var s = document.getElementsByTagName('script')[0];
                    s.parentNode.insertBefore(gcse, s);
                })();
            </script>
            <gcse:searchbox-only></gcse:searchbox-only>
        </div>
    </div>
</div>

<div class="col-md-9 col-md-pull-3">
    <h1 id="feature-top" class="title">Features</h1>

    <p>We have went through the classification and regression algorithms. In the examples,
        we assume that the data is ready and features are carefully prepared, which many
        machine learning practitioners take as granted. However, it is not general case
        in practice. The practitioners should pay a lot attentions on feature generation,
        selection, and dimension reduction, which often have bigger impacts on the final
        results than the choice of particular learning algorithm.</p>

    <p>Understanding the problem (and the business) is the most important thing to prepare
        the features. Besides the attributes supplied with the training instances, researchers
        should modify or enhance the representation of data objects in search of new features
        that describe the objects better.</p>

    <p>The accuracy of the inferred function depends strongly on how the input
        object is represented. Typically, the input object is transformed into
        a feature vector, which contains a number of features that are descriptive
        of the object. The number of features should not be too large, because of
        the curse of dimensionality; but should contain enough information to
        accurately predict the output. Beyond feature vectors, a few algorithms such as support vector machines
        can process complex data object such as sequences, trees, and even graphs with
        properly engineered kernels.</p>

    <p>Many machine learning methods such as Neural Networks and SVM with Gaussian
        kernel also require the features properly scaled/standardized. For example,
        each variable is scaled into interval [0, 1] or to have mean 0 and standard
        deviation 1. Methods that employ a distance function
        are particularly sensitive to this. Although some method such as decision trees can handle nominal
        variable directly, other methods generally require nominal variables converted
        to multiple binary dummy variables to indicate the presence or absence of a
        characteristic.</p>

    <p>If the input features contain redundant information (e.g., highly correlated
        features), some learning algorithms (e.g., linear regression, logistic
        regression, and distance based methods) will perform poorly because of
        numerical instabilities. These problems can often be solved by imposing
        some form of regularization.</p>

    <p>If each of the features makes an independent contribution to the output,
        then algorithms based on linear functions (e.g., linear regression,
        logistic regression, linear support vector machines, naive Bayes) generally
        perform well. However, if there are complex interactions among features,
        then algorithms such as nonlinear support vector machines, decision trees
        and neural networks work better. Linear methods can also be applied, but
        engineers must manually specify the interactions when using them.</p>

    <p>In practice, we often start with generating a lot of features with the hope that
        more relevant information will improve the accuracy. However, it is not always
        good to include a large number of features in the learning system.
        It is well known that the optimal rate of convergence to fit the
        <code>m</code>-th derivate of &theta; (&theta; is a <code>p</code>-times differentiable regression
        function, especially nonparametric ones) to the data is at best proportional
        to <code>n<sup>-(p-m)/(2p+d)</sup></code>, where <code>n</code> is the number
        of data points, and <code>d</code> is the dimensionality of the data.
        In a nutshell, the rate of convergence will be exponentially slower
        when we increase the dimensionality of our inputs. In other words,
        with a fixed number of training samples, the predictive power reduces
        as the dimensionality increases, known as the Hughes effect.</p>

    <p>Therefore, feature selection that identifies
        the relevant features and discards the irrelevant ones and dimension reduction
        that maps the input data into a lower
        dimensional space are generally required prior to running the supervised learning algorithm.</p>

    <h2 id="feature-selection">Feature Selection</h2>

    <p>Feature selection is the technique of selecting a subset of relevant
        features for building robust learning models. By removing most irrelevant
        and redundant features from the data, feature selection helps improve the
        performance of learning models by alleviating the effect of the curse of
        dimensionality, enhancing generalization capability, speeding up learning
        process, etc. More importantly, feature selection also helps researchers
        to acquire better understanding about the data.</p>

    <p>Feature selection algorithms typically fall into two categories: feature
        ranking and subset selection. Feature ranking methods rank the features by a
        metric and eliminate all features that do not achieve an adequate score.
        Subset selection searches the set of possible features for the optimal subset.
        Clearly, an exhaustive search of optimal subset is impractical if large
        numbers of features are available. Commonly, heuristic methods such as
        genetic algorithms are employed for subset selection.</p>

    <h3 id="signal-noise-ratio">Signal Noise Ratio</h3>

    <p>The signal-to-noise (S2N) metric ratio is a univariate feature ranking metric,
        which can be used as a feature selection criterion for binary classification
        problems. S2N is defined as</p>

    <pre class="prettyprint lang-html"><code>
    |&mu;<sub>1</sub> - &mu;<sub>2</sub>| / (&sigma;<sub>1</sub> + &sigma;<sub>2</sub>)
    </code></pre>

    <p>where &mu;<sub>1</sub> and &mu;<sub>2</sub> are the mean value of the variable
        in classes 1 and 2, respectively, and &sigma;<sub>1</sub> and &sigma;<sub>2</sub>
        are the standard deviations of the variable in classes 1 and 2, respectively.
        Clearly, features with larger S2N ratios are better for classification.</p>

    <pre class="prettyprint lang-scala"><code>
    def signalNoiseRatio(x: Array[Array[Double]], y: Array[Int]): Array[Double]
    </code></pre>

    <p>The output is the S2N ratios for each variable.</p>

    <h3 id="sum-squares-ratio">Sum Squares Ratio</h3>

    <p>The ratio of between-groups to within-groups sum of squares is a univariate
        feature ranking metric, which can be used as a feature selection criterion
        for multi-class classification problems. For each variable j, this ratio is</p>

    <pre class="prettyprint lang-html"><code>
    BSS(j) / WSS(j) = &Sigma;I(y<sub>i</sub> = k)(x<sub>kj</sub> - x<sub>&middot;j</sub>)<sup>2</sup> / &Sigma;I(y<sub>i</sub> = k)(x<sub>ij</sub> - x<sub>kj</sub>)<sup>2</sup>
    </code></pre>

    <p>where x<sub>&middot;j</sub> denotes the average of variable j across all
        samples, x<sub>kj</sub> denotes the average of variable j across samples
        belonging to class k, and x<sub>ij</sub> is the value of variable j of sample i.
        Clearly, features with larger sum squares ratios are better for classification.</p>

    <pre class="prettyprint lang-scala"><code>
    def sumSquaresRatio(x: Array[Array[Double]], y: Array[Int]): Array[Double]
    </code></pre>

    <p>Applying it to Iris data, we can find that the last two variables have much higher
        sum squares ratios (about 16 and 13 in contrast to 1.6 and 0.6 of the first two variables).</p>

    <pre class="prettyprint lang-scala"><code>
    val iris = readArff("data/weka/iris.arff", 4)
    val (x, y) = iris.unzipInt
    sumSquaresRatio(x, y)

    val x2 = x.map(_.slice(2,4)) // take last two columns
    val window = plot(x2, y, Array('*', '+', 'o'), Array(Color.RED, Color.BLUE, Color.CYAN))
    window.canvas.setAxisLabels(iris.attributes.map(_.getName).slice(2,4): _*)
    </code></pre>

    <p>The scatter plot of the last two variables shows clearly that they capture the difference
        among classes.</p>

    <div style="width: 100%; display: inline-block; text-align: center;">
        <img src="images/iris-petal-width-length.png" class="enlarge" style="width: 480px;" />
        <div class="caption" style="min-width: 480px;">Iris Petal Width vs Length</div>
    </div>

    <h3 id="ensemble-learning-feature-selection">Ensemble Learning Based Feature Selection</h3>

    <p>Ensemble learning methods (e.g. random forest, gradient boosted trees, and AdaBoost) can give
        the estimates of what variables are important in the classification.
        Every time a split of a node is made on variable
        the (GINI, information gain, etc.) impurity criterion for the two
        descendent nodes is less than the parent node. Adding up the decreases
        for each individual variable over all trees in the forest gives a fast
        variable importance that is often very consistent with the permutation
        importance measure.</p>

    <p>For these algorithms, there is a method <code>importance</code> returning
        the scores for feature selection (higher is better).</p>

    <pre class="prettyprint lang-scala"><code>
    val model = randomForest(x, y)
    println(model.importance.mkString(", ")
    </code></pre>

    <p>For Iris data, random forest also gives much higher importance scores for
        the last two variables.</p>

    <p>For data including categorical variables with different number of
        levels, random forests are biased in favor of those attributes with more
        levels. Therefore, the variable importance scores from random forest are
        not reliable for this type of data.</p>

    <h3 id="genetic-algorithm-feature-selection">Genetic Algorithm Based Feature Selection</h3>

    <p>Genetic algorithm (GA) is a search heuristic that mimics the process of
        natural evolution. This heuristic is routinely used to generate useful
        solutions to optimization and search problems. Genetic algorithms belong
        to the larger class of evolutionary algorithms (EA), which generate solutions
        to optimization problems using techniques inspired by natural evolution,
        such as inheritance, mutation, selection, and crossover.</p>

    <p>In a genetic algorithm, a population of strings (called chromosomes), which
        encode candidate solutions (called individuals) to an optimization problem,
        evolves toward better solutions. Traditionally, solutions are represented in binary as strings
        of 0s and 1s, but other encodings are also possible. The evolution usually
        starts from a population of randomly generated individuals and happens in
        generations. In each generation, the fitness of every individual in the
        population is evaluated, multiple individuals are stochastically selected
        from the current population (based on their fitness), and modified
        (recombined and possibly randomly mutated) to form a new population. The
        new population is then used in the next iteration of the algorithm. Commonly,
        the algorithm terminates when either a maximum number of generations has been
        produced, or a satisfactory fitness level has been reached for the population.
        If the algorithm has terminated due to a maximum number of generations, a
        satisfactory solution may or may not have been reached.</p>

    <p>Genetic algorithm based feature selection finds many (random)
        subsets of variables of expected classification power.
        The "fitness" of each subset of variables is determined by its
        ability to classify the samples according to a given classification
        method.</p>

    <p>In the below example, we use gradient boosted trees (100 tress)
        as the classifier, the accuracy of 5-fold cross validation as the fitness measure.</p>

    <pre class="prettyprint lang-java"><code>
    val data = readArff("data/weka/segment-test.arff", 19)
    val (x, y) = data.unzipInt

    val trainer = new GradientTreeBoost.Trainer(100)
    val measure = new Accuracy

    // Use default settings for Genetic Algorithm.
    val selector = new GAFeatureSelection

    // The population size is 50, and 20 generation evolution.
    // Each returned BitString contains the selected feature subset and its fitness.
    val result = selector.learn(50, 20, trainer, measure, x, y, 5)

    result.foreach { bits =>
        print(f"${100*bits.fitness}%.2f%% ")
        println(bits.bits.mkString(" "))
    }
    </code></pre>

    <p>When many such subsets of variables are obtained, the one with best
        performance may be used as selected features. Alternatively, the frequencies
        with which variables are selected may be analyzed further. The most
        frequently selected variables may be presumed to be the most relevant to
        sample distinction and are finally used for prediction.</p>

    <p>Although GA avoids brute-force search, it is still much slower than univariate feature selection.</p>

    <h2 id="dimension-reduction">Dimension Reduction</h2>

    <p>Dimension Reduction, also called Feature Extraction, transforms the data in the
        high-dimensional space to a space of fewer dimensions. The data
        transformation may be linear, as in principal component analysis (PCA),
        but many nonlinear dimensionality reduction techniques also exist.</p>

    <h3 id="pca">Principal Component Analysis</h3>

    <p>The main linear technique for dimensionality reduction, principal component
        analysis, performs a linear mapping of the data to a lower dimensional
        space in such a way that the variance of the data in the low-dimensional
        representation is maximized. In practice, the correlation matrix of the
        data is constructed and the eigenvectors on this matrix are computed.
        The eigenvectors that correspond to the largest eigenvalues (the principal
        components) can now be used to reconstruct a large fraction of the variance
        of the original data. Moreover, the first few eigenvectors can often be
        interpreted in terms of the large-scale physical behavior of the system.
        The original space has been reduced (with data loss, but hopefully
        retaining the most important variance) to the space spanned by a few
        eigenvectors.</p>

    <pre class="prettyprint lang-scala"><code>
    def pca(data: Array[Array[Double]], cor: Boolean = false): PCA
    </code></pre>

    <p>If the parameter <code>cor</code> is true, PCA uses correlation matrix instead of covariance matrix.
        The correlation matrix is simply the covariance matrix, standardized by setting all variances equal
        to one. When scales of variables are similar, the covariance matrix is always preferred,
        as the correlation matrix will lose information when standardizing the variance.
        The correlation matrix is recommended when variables are measured in different scales.</p>

    <pre class="prettyprint lang-scala"><code>
    val pendigits = readTable("data/classification/pendigits.txt", Some((new NominalAttribute("class"), 16)))
    val (x, y) = pendigits.unzipInt
    val pc = pca(x)
    screeplot(pc)

    pc.setProjection(3)
    val x2 = pc.project(x)
    plot(x2, y, '.', Palette.COLORS)
    </code></pre>

    <p>In the above example, we apply PCA to the pen-digits data, which includes 16 variables.</p>

    <div style="width: 100%; display: inline-block; text-align: center;">
        <img src="images/scree.png" class="enlarge" style="width: 480px;" />
        <div class="caption" style="min-width: 480px;">Scree Plot</div>
    </div>

    <p>The scree plot is a useful visual aid for determining an appropriate number of principal components.
        The scree plot graphs the eigenvalue against the component number. To determine the appropriate
        number of components, we look for an "elbow" in the scree plot. The component number is taken to
        be the point at which the remaining eigenvalues are relatively small and all about the same size.</p>

    <p>Finally, we plot the data projected into 3-dimensional space.</p>

    <div style="width: 100%; display: inline-block; text-align: center;">
        <img src="images/pca.png" class="enlarge" style="width: 480px;" />
        <div class="caption" style="min-width: 480px;">PCA</div>
    </div>

    <h3 id="ppca">Probabilistic Principal Component Analysis</h3>

    <p>Probabilistic principal component analysis is a simplified factor analysis
        that employs a latent variable model with linear relationship:

    <pre class="prettyprint lang-html"><code>
    y &sim; W * x + &mu; + &epsilon;
    </code></pre>

    <p>where latent variables x &sim; N(0, I), error (or noise) &epsilon; &sim; N(0, &Psi;),
        and &mu; is the location term (mean). In probabilistic PCA, an isotropic noise model is used,
        i.e., noise variances constrained to be equal (&Psi;<sub>i</sub> = &sigma;<sup>2</sup>).
        A close form of estimation of above parameters can be obtained
        by maximum likelihood method.</p>

    <pre class="prettyprint lang-scala"><code>
    def ppca(data: Array[Array[Double]], k: Int): PPCA
    </code></pre>

    <p>Similar to PCA example, we apply probabilistic PCA to the pen-digits data.</p>

    <pre class="prettyprint lang-scala"><code>
    val pc = ppca(x, 3)
    val x2 = pc.project(x)
    plot(x2, y, '.', Palette.COLORS)
    </code></pre>

    <div style="width: 100%; display: inline-block; text-align: center;">
        <img src="images/ppca.png" class="enlarge" style="width: 480px;" />
        <div class="caption" style="min-width: 480px;">Probabilistic PCA</div>
    </div>

    <h3 id="kpca">Kernel Principal Component Analysis</h3>

    <p>Principal component analysis can be employed in a nonlinear way by means
        of the kernel trick. The resulting technique is capable of constructing
        nonlinear mappings that maximize the variance in the data. The resulting
        technique is entitled Kernel PCA. Other prominent nonlinear techniques
        include manifold learning techniques such as locally linear embedding
        (LLE), Hessian LLE, Laplacian eigenmaps, and LTSA. These techniques
        construct a low-dimensional data representation using a cost function
        that retains local properties of the data, and can be viewed as defining
        a graph-based kernel for Kernel PCA.</p>

    <p>In practice, a large data set leads to a large Kernel/Gram matrix <code>K</code>, and
        storing K may become a problem. One way to deal with this is to perform
        clustering on your large dataset, and populate the kernel with the means
        of those clusters. Since even this method may yield a relatively large K,
        it is common to compute only the top P eigenvalues and eigenvectors of <code>K</code>.</p>

    <pre class="prettyprint lang-scala"><code>
    def kpca[T](data: Array[T], kernel: MercerKernel[T], k: Int, threshold: Double = 0.0001): KPCA[T]
    </code></pre>

    <p>Here we apply kernel PCA to the pen-digits data.</p>

    <pre class="prettyprint lang-scala"><code>
    val pc = kpca(x, new GaussianKernel(45), 3)
    val x2 = pc.project(x)
    plot(x2, y, '.', Palette.COLORS)
    </code></pre>

    <p>Because of its nonlinear nature, the projected data has very different structure from PCA.</p>

    <div style="width: 100%; display: inline-block; text-align: center;">
        <img src="images/kpca.png" class="enlarge" style="width: 480px;" />
        <div class="caption" style="min-width: 480px;">Kernel PCA</div>
    </div>

    <h3 id="gha">Generalized Hebbian Algorithm</h3>

    <p>Compared to regular batch PCA algorithm, the generalized Hebbian algorithm (GHA)
        is an adaptive method to find the largest k eigenvectors of the covariance
        matrix, assuming that the associated eigenvalues are distinct. GHA works
        with an arbitrarily large sample size and the storage requirement is modest.
        Another attractive feature is that, in a nonstationary environment, it
        has an inherent ability to track gradual changes in the optimal solution
        in an inexpensive way.</p>

    <p>It guarantees that GHA finds the first k eigenvectors of the covariance matrix,
        assuming that the associated eigenvalues are distinct. The convergence theorem
        is formulated in terms of a time-varying learning rate &eta;. In practice, the
        learning rate &eta; is chosen to be a small constant, in which case convergence is
        guaranteed with mean-squared error in synaptic weights of order &eta;.</p>

    <p>It also has a simple and predictable trade-off between learning speed and
        accuracy of convergence as set by the learning rate parameter &eta;. It was
        shown that a larger learning rate &eta; leads to faster convergence
        and larger asymptotic mean-square error, which is intuitively satisfying.</p>

    <pre class="prettyprint lang-scala"><code>
    def gha(data: Array[Array[Double]], k: Int, r: Double): GHA

    def gha(data: Array[Array[Double]], w: Array[Array[Double]], r: Double): GHA
    </code></pre>

    <p>The first method starts with random initial projection matrix, while the second one
        starts with a given projection matrix.</p>

    <pre class="prettyprint lang-scala"><code>
    val pc = gha(x, 3, 0.00001)
    val x2 = pc.project(x)
    plot(x2, y, '.', Palette.COLORS)
    </code></pre>

    <p>The results look similar to PCA.</p>
    
    <div style="width: 100%; display: inline-block; text-align: center;">
        <img src="images/gha.png" class="enlarge" style="width: 480px;" />
        <div class="caption" style="min-width: 480px;">GHA</div>
    </div>

    <div id="btnv">
        <span class="btn-arrow-left">&larr; &nbsp;</span>
        <a class="btn-prev-text" id="regression" href="#" title="Previous Section: Regression"><span>Regression</span></a>
        <a class="btn-next-text" id="validation" href="#" title="Next Section: Model Validation"><span>Model Validation</span></a>
        <span class="btn-arrow-right">&nbsp;&rarr;</span>
    </div>
</div>

<script type="text/javascript">
    $('#toc').toc({exclude: 'h1, h5, h6', context: '', autoId: true, numerate: false});
</script>