---
layout: layout.njk
permalink: "{{ page.filePathStem }}.html"
---
{% include "toc.njk" %}

<div class="col-md-9 col-md-pull-3">
    <h1 id="data-top" class="title">Data</h1>

    <p>Machine learning is all about building models from data. However, data scientists
        frequently talk about models and algorithms first, which very likely generates
        suboptimal results. The other approach is to play with the data first. Even simple
        statistics and plots can help us get feelings of data and problems, which more likely
        lead us to better modelling.</p>

    <h2 id="features">Features</h2>

    <p>A feature is an individual measurable property of a phenomenon being observed.
        Features are also called explanatory variables, independent variables, predictors, regressors, etc.
        Any attribute could be a feature, but choosing informative, discriminating and
        independent features is a crucial step for effective algorithms in machine learning.
        Features are usually numeric and a set of numeric features can be conveniently
        described by a feature vector. Structural features such as strings, sequences and
        graphs are also used in areas such as natural language processing, computational biology, etc.</p>

    <p>Feature engineering is the process of using domain knowledge of the data to create features that make
        machine learning algorithms work. Feature engineering is fundamental to the application of machine
        learning, and is both difficult and expensive. It requires the experimentation of multiple
        possibilities and the combination of automated
        techniques with the intuition and knowledge of the domain expert.</p>

    <h2 title="data-type">Data Type</h2>
    <p>Generally speaking, there are two major types of attributes:</p>
        <dl>
            <dt>Qualitative variables:</dt>
            <dd><p>The data values are non-numeric categories. Examples: Blood type, Gender.</p></dd>
            <dt>Quantitative variables:</dt>
            <dd><p>The data values are counts or numerical measurements. A quantitative
                variable can be either discrete such as the number of students receiving
                an 'A' in a class, or continuous such as GPA, salary and so on.</p></dd>
        </dl>

    <p>Another way of classifying data is by the measurement scales. In statistics,
        there are four generally used measurement scales:</p>

        <dl>
            <dt>Nominal data:</dt>
            <dd><p>Data values are non-numeric group labels. For example, Gender variable
                can be defined as male = 0 and female =1.</p></dd>
            <dt>Ordinal data:</dt>
            <dd><p>Data values are categorical and may be ranked in some numerically
                meaningful way. For example, strongly disagree to strong agree may be
                defined as 1 to 5.</p></dd>
            <dt>Continuous data:</dt>
            <dd><ul>
                <li><strong>Interval data:</strong>
                Data values are ranged in a real interval, which can be as large as
                from negative infinity to positive infinity. The difference between two
                values are meaningful, however, the ratio of two interval data is not
                meaningful. For example temperature, IQ. </li>
                <li><strong>Ratio data:</strong>
                Both difference and ratio of two values are meaningful. For example,
                salary, weight.</li>
            </ul></dd>
        </dl>

    <p>Many machine learning algorithms can only handle numeric attributes while a few
        such as decision trees can process nominal attribute directly. Date attribute
        is useful in plotting. With some feature engineering, values like day of week
        can be used as nominal attribute. String attribute could be used in text mining
        and natural language processing.</p>

    <h2 id="DataFrame">DataFrame</h2>

    <p>Many Smile algorithms take simple <code>double[]</code> as input. But we also use
        the encapsulation class <a href="api/java/smile/data/DataFrame.html">DataFrame</a>.
        In fact, the output of most Smile data parsers is a DataFrame object.
        DataFrames are immutable and contain a fixed number of named columns.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> val iris = read.arff("data/weka/iris.arff")
    [main] INFO smile.io.Arff - Read ARFF relation iris
    iris: DataFrame =
    +-----------+----------+-----------+----------+-----------+
    |sepallength|sepalwidth|petallength|petalwidth|      class|
    +-----------+----------+-----------+----------+-----------+
    |        5.1|       3.5|        1.4|       0.2|Iris-setosa|
    |        4.9|         3|        1.4|       0.2|Iris-setosa|
    |        4.7|       3.2|        1.3|       0.2|Iris-setosa|
    |        4.6|       3.1|        1.5|       0.2|Iris-setosa|
    |          5|       3.6|        1.4|       0.2|Iris-setosa|
    |        5.4|       3.9|        1.7|       0.4|Iris-setosa|
    |        4.6|       3.4|        1.4|       0.3|Iris-setosa|
    |          5|       3.4|        1.5|       0.2|Iris-setosa|
    |        4.4|       2.9|        1.4|       0.2|Iris-setosa|
    |        4.9|       3.1|        1.5|       0.1|Iris-setosa|
    +-----------+----------+-----------+----------+-----------+
    140 more rows...

    smile> iris.summary
    res1: DataFrame =
    +-----------+-----+---+--------+---+
    |     column|count|min|     avg|max|
    +-----------+-----+---+--------+---+
    |sepallength|  150|4.3|5.843333|7.9|
    | sepalwidth|  150|  2|   3.054|4.4|
    |petallength|  150|  1|3.758667|6.9|
    | petalwidth|  150|0.1|1.198667|2.5|
    +-----------+-----+---+--------+---+
    </code></pre>

    <p>We can get a row with the array syntax or refer a column by its name.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> iris(0)
    res5: Tuple = {
      sepallength: 5.1,
      sepalwidth: 3.5,
      petallength: 1.4,
      petalwidth: 0.2,
      class: Iris-setosa
    }

    smile> iris("sepallength")
    res6: vector.BaseVector[T, TS, S] = [5.099999904632568, 4.900000095367432, 4.699999809265137, 4.599999904632568, 5.0, 5.400000095367432, 4.599999904632568, 5.0, 4.400000095367432, 4.900000095367432, ... 140 more]
    </code></pre>

    <p>Similarly, we can select a few columns to create a new data frame. </p>
    <pre class="prettyprint lang-scala"><code>
    smile> iris.select("sepallength", "sepalwidth")
    res8: DataFrame =
    +-----------+----------+
    |sepallength|sepalwidth|
    +-----------+----------+
    |        5.1|       3.5|
    |        4.9|         3|
    |        4.7|       3.2|
    |        4.6|       3.1|
    |          5|       3.6|
    |        5.4|       3.9|
    |        4.6|       3.4|
    |          5|       3.4|
    |        4.4|       2.9|
    |        4.9|       3.1|
    +-----------+----------+
    140 more rows...
    </code></pre>

    <p>Advanced operations such as <code>exists</code>, <code>forall</code>,
        <code>find</code>, <code>filter</code> are also supported.
        The <code>predicate</code> of these functions expect a <code>Tuple</code></p>
    <pre class="prettyprint lang-scala"><code>
    smile&gt; iris.exists(_.getDouble(0) > 4.5)
    res16: Boolean = true

    smile&gt; iris.forall(_.getDouble(0) < 10)
    res17: Boolean = true

    smile&gt; iris.find(_("class") == 1)
    res18: java.util.Optional[Tuple] = Optional[{
      sepallength: 6.2,
      sepalwidth: 2.9,
      petallength: 4.3,
      petalwidth: 1.3,
      class: Iris-versicolor
    }]

    smile&gt; iris.find(_.getString("class").equals("Iris-versicolor"))
    res19: java.util.Optional[Tuple] = Optional[{
      sepallength: 6.2,
      sepalwidth: 2.9,
      petallength: 4.3,
      petalwidth: 1.3,
      class: Iris-versicolor
    }]

    smile&gt; iris.filter { row => row.getDouble(1) > 3 && row("class") != 0 }
    res20: DataFrame =
    +-----------+----------+-----------+----------+---------------+
    |sepallength|sepalwidth|petallength|petalwidth|          class|
    +-----------+----------+-----------+----------+---------------+
    |          7|       3.2|        4.7|       1.4|Iris-versicolor|
    |        6.4|       3.2|        4.5|       1.5|Iris-versicolor|
    |        6.9|       3.1|        4.9|       1.5|Iris-versicolor|
    |        6.3|       3.3|        4.7|       1.6|Iris-versicolor|
    |        6.7|       3.1|        4.4|       1.4|Iris-versicolor|
    |        5.9|       3.2|        4.8|       1.8|Iris-versicolor|
    |          6|       3.4|        4.5|       1.6|Iris-versicolor|
    |        6.7|       3.1|        4.7|       1.5|Iris-versicolor|
    |        6.3|       3.3|          6|       2.5| Iris-virginica|
    |        7.2|       3.6|        6.1|       2.5| Iris-virginica|
    +-----------+----------+-----------+----------+---------------+
    15 more rows...
    </code></pre>

    <p>Besides numeric and nominal values, many other data types
        are also supported.</p>
    <pre class="prettyprint lang-scala"><code>
    smile> val strings = read.arff("data/weka/string.arff")
    [main] INFO smile.io.Arff - Read ARFF relation LCCvsLCSH
    strings: DataFrame =
    +-----+--------------------------------------+
    |  LCC|                                  LCSH|
    +-----+--------------------------------------+
    |  AG5|Encyclopedias and dictionaries.;Twe...|
    |AS262|   Science -- Soviet Union -- History.|
    |  AE5|       Encyclopedias and dictionaries.|
    |AS281|Astronomy, Assyro-Babylonian.;Moon ...|
    |AS281|Astronomy, Assyro-Babylonian.;Moon ...|
    +-----+--------------------------------------+

    smile> strings.filter(_.getString(0).startsWith("AS"))
    res21: DataFrame =
    +-----+--------------------------------------+
    |  LCC|                                  LCSH|
    +-----+--------------------------------------+
    |AS262|   Science -- Soviet Union -- History.|
    |AS281|Astronomy, Assyro-Babylonian.;Moon ...|
    |AS281|Astronomy, Assyro-Babylonian.;Moon ...|
    +-----+--------------------------------------+

    smile> val dates = read.arff("data/weka/date.arff")
    [main] INFO smile.io.Arff - Read ARFF relation Timestamps
    dates: DataFrame =
    +-------------------+
    |          timestamp|
    +-------------------+
    |2001-04-03 12:12:12|
    |2001-05-03 12:59:55|
    +-------------------+
    </code></pre>

    <p>For data wrangling, the most important functions of <code>DataFrame</code>
        are <code>map</code> and <code>groupBy</code>.</p>

    <pre class="prettyprint lang-scala"><code>
    smile&gt; iris.map { row =>
                    val x = new Array[Double](6)
                    for (i <- 0 until 4) x(i) = row.getDouble(i)
                    x(4) = x(0) * x(1)
                    x(5) = x(2) * x(3)
                    x
                  }
    res22: Iterable[Array[Double]] = ArrayBuffer(
      Array(5.1, 3.5, 1.4, 0.2, 17.849999999999998, 0.27999999999999997),
      Array(4.9, 3.0, 1.4, 0.2, 14.700000000000001, 0.27999999999999997),
      Array(4.7, 3.2, 1.3, 0.2, 15.040000000000001, 0.26),
      Array(4.6, 3.1, 1.5, 0.2, 14.26, 0.30000000000000004),
      Array(5.0, 3.6, 1.4, 0.2, 18.0, 0.27999999999999997),
      Array(5.4, 3.9, 1.7, 0.4, 21.060000000000002, 0.68),
      Array(4.6, 3.4, 1.4, 0.3, 15.639999999999999, 0.42),
      Array(5.0, 3.4, 1.5, 0.2, 17.0, 0.30000000000000004),
      Array(4.4, 2.9, 1.4, 0.2, 12.76, 0.27999999999999997),
      Array(4.9, 3.1, 1.5, 0.1, 15.190000000000001, 0.15000000000000002),
      Array(5.4, 3.7, 1.5, 0.2, 19.980000000000004, 0.30000000000000004),
      Array(4.8, 3.4, 1.6, 0.2, 16.32, 0.32000000000000006),
      Array(4.8, 3.0, 1.4, 0.1, 14.399999999999999, 0.13999999999999999),
      Array(4.3, 3.0, 1.1, 0.1, 12.899999999999999, 0.11000000000000001),
      Array(5.8, 4.0, 1.2, 0.2, 23.2, 0.24),
      Array(5.7, 4.4, 1.5, 0.4, 25.080000000000002, 0.6000000000000001),
      Array(5.4, 3.9, 1.3, 0.4, 21.060000000000002, 0.52),
      Array(5.1, 3.5, 1.4, 0.3, 17.849999999999998, 0.42),
      Array(5.7, 3.8, 1.7, 0.3, 21.66, 0.51),
      Array(5.1, 3.8, 1.5, 0.3, 19.38, 0.44999999999999996),
      Array(5.4, 3.4, 1.7, 0.2, 18.36, 0.34),
      Array(5.1, 3.7, 1.5, 0.4, 18.87, 0.6000000000000001),
      Array(4.6, 3.6, 1.0, 0.2, 16.56, 0.2),
      Array(5.1, 3.3, 1.7, 0.5, 16.83, 0.85),
    ...
    </code></pre>

    <pre class="prettyprint lang-scala"><code>
    smile> iris.groupBy(row => row.getString("class"))
    res23: Map[String, DataFrame] = Map(
      "Iris-virginica" ->
    +-----------+----------+-----------+----------+--------------+
    |sepallength|sepalwidth|petallength|petalwidth|         class|
    +-----------+----------+-----------+----------+--------------+
    |        6.3|       3.3|          6|       2.5|Iris-virginica|
    |        5.8|       2.7|        5.1|       1.9|Iris-virginica|
    |        7.1|         3|        5.9|       2.1|Iris-virginica|
    |        6.3|       2.9|        5.6|       1.8|Iris-virginica|
    |        6.5|         3|        5.8|       2.2|Iris-virginica|
    |        7.6|         3|        6.6|       2.1|Iris-virginica|
    |        4.9|       2.5|        4.5|       1.7|Iris-virginica|
    |        7.3|       2.9|        6.3|       1.8|Iris-virginica|
    |        6.7|       2.5|        5.8|       1.8|Iris-virginica|
    |        7.2|       3.6|        6.1|       2.5|Iris-virginica|
    +-----------+----------+-----------+----------+--------------+
    40 more rows...
    ,
      "Iris-versicolor" ->
    +-----------+----------+-----------+----------+---------------+
    |sepallength|sepalwidth|petallength|petalwidth|          class|
    +-----------+----------+-----------+----------+---------------+
    |          7|       3.2|        4.7|       1.4|Iris-versicolor|
    |        6.4|       3.2|        4.5|       1.5|Iris-versicolor|
    |        6.9|       3.1|        4.9|       1.5|Iris-versicolor|
    ...
    </code></pre>

    <h2 id="sparse">Sparse Dataset</h2>

    <p>The feature vectors could be very sparse. To save space, <a href="api/java/smile/data/SparseDataset.html">SparseDataset</a>
        stores data in a list of lists (LIL) sparse matrix format. SparseDataset stores one list
        per row, where each entry stores a column index and value. Typically, these entries
        are kept sorted by column index for faster lookup.</p>

    <p>SparseDataset is often used to construct the data matrix. Once the matrix is constructed,
        it is typically converted to a format, such as <a href="api/java/smile/math/matrix/SparseMatrix.html">Harwell-Boeing</a>
        column-compressed sparse matrix format, which is more efficient for matrix operations.</p>

    <p>The class <a href="api/java/smile/data/BinarySparseDataset.html">BinarySparseDataset</a> is more efficient for
        binary sparse data. In BinarySparseDataset, each item is stored as an integer array, which are
        the indices of nonzero elements in ascending order.</p>

    <h2 id="parser">Parsers</h2>

    <p>Smile provides a couple of parsers for popular data formats, such as Parquet, Avro, Arrow, SAS7BDAT, Weka's ARFF files,
        LibSVM's file format, delimited text files, JSON, and binary sparse data. We will demonstrate
        these parsers with the sample data in the <code>data</code> directory. In Scala API, the
        parsing functions are in the <code>smile.read</code> object.</p>

    <h3 id="read.parquet">Apache Parquet</h3>
    <p><a href="https://parquet.apache.org/">Apache Parquet</a>
        is a columnar storage format that supports
        nested data structures. It uses the record shredding and
        assembly algorithm described in the Dremel paper.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> val df = read.parquet("shell/src/universal/data/parquet/userdata1.parquet")
    df: DataFrame = [registration_dttm: DateTime, id: Integer, first_name: String, last_name: String, email: String, gender: String, ip_address: String, cc: String, country: String, birthdate: String, salary: Double, title: String, comments: String]
    +-------------------+---+----------+---------+--------------------+------+--------------+----------------+--------------------+----------+---------+--------------------+--------+
    |  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|             country| birthdate|   salary|               title|comments|
    +-------------------+---+----------+---------+--------------------+------+--------------+----------------+--------------------+----------+---------+--------------------+--------+
    |2016-02-03T07:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|   1E+02|
    |2016-02-03T17:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |              Canada| 1/16/1968|150280.17|       Accountant IV|        |
    |2016-02-03T01:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|        |
    |2016-02-03T00:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female| 140.35.109.83|3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|        |
    |2016-02-03T05:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      |169.113.235.40|5602256255204850|        South Africa|          |     null|                    |        |
    |2016-02-03T07:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female|195.131.81.179|3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|        |
    |2016-02-03T08:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male|232.234.81.197|3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|        |
    |2016-02-03T06:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|  91.235.51.73|                |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|        |
    ...
    </code></pre>

    <h3 id="read.avro">Apache Avro</h3>
    <p><a href="https://avro.apache.org/">Apache Avro</a>
        is a data serialization system.
        Avro provides rich data structures, a compact, fast, binary data format,
        a container file, to store persistent data, and remote procedure call (RPC).
        Avro relies on schemas. When Avro data is stored in a file, its schema
        is stored with it. Avro schemas are defined with JSON.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> val stream = java.nio.file.Files.newInputStream(Paths.getTestData("avro/userdata.avsc"))
    stream: java.io.InputStream = sun.nio.ch.ChannelInputStream@1e5e2e06

    smile> val schema = new org.apache.avro.Schema.Parser().parse(stream)
    schema: org.apache.avro.Schema = {"type":"record","name":"kylosample","doc":"Schema generated by Kite","fields":[{"name":"registration_dttm","type":"string","doc":"Type inferred from '2016-02-03T07:55:29Z'"},{"name":"id","type":"long","doc":"Type inferred from '1'"},{"name":"first_name","type":"string","doc":"Type inferred from 'Amanda'"},{"name":"last_name","type":"string","doc":"Type inferred from 'Jordan'"},{"name":"email","type":"string","doc":"Type inferred from 'ajordan0@com.com'"},{"name":"gender","type":"string","doc":"Type inferred from 'Female'"},{"name":"ip_address","type":"string","doc":"Type inferred from '1.197.201.2'"},{"name":"cc","type":["null","long"],"doc":"Type inferred from '6759521864920116'","default":null},{"name":"country","type":"string","doc":"Type inferred from 'Indonesia'"},{"name":"birthdate","type":"string","doc":"Type inferred from '3/8/1971'"},{"name":"salary","type":["null","double"],"doc":"Type inferred from '49756.53'","default":null},{"name":"title","type":"string","doc":"Type inferred from 'Internal Auditor'"},{"name":"comments","type":"string","doc":"Type inferred from '1E+02'"}]}

    smile> val df = read.avro(Paths.getTestData("avro/userdata1.avro"), schema)
    df: DataFrame = [registration_dttm: DateTime, id: Integer, first_name: String, last_name: String, email: String, gender: String, ip_address: String, cc: String, country: String, birthdate: String, salary: Double, title: String, comments: String]
    +-------------------+---+----------+---------+--------------------+------+--------------+----------------+--------------------+----------+---------+--------------------+--------+
    |  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|             country| birthdate|   salary|               title|comments|
    +-------------------+---+----------+---------+--------------------+------+--------------+----------------+--------------------+----------+---------+--------------------+--------+
    |2016-02-03T07:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|   1E+02|
    |2016-02-03T17:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |              Canada| 1/16/1968|150280.17|       Accountant IV|        |
    |2016-02-03T01:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|        |
    |2016-02-03T00:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female| 140.35.109.83|3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|        |
    |2016-02-03T05:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      |169.113.235.40|5602256255204850|        South Africa|          |     null|                    |        |
    |2016-02-03T07:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female|195.131.81.179|3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|        |
    |2016-02-03T08:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male|232.234.81.197|3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|        |
    |2016-02-03T06:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|  91.235.51.73|                |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|        |
    ...
    </code></pre>

    <h3 id="read.arrow">Apache Arrow</h3>
    <p><a href="https://arrow.apache.org/">Apache Arrow</a>
        is a cross-language development platform for in-memory data.
        It specifies a standardized language-independent columnar memory format
        for flat and hierarchical data, organized for efficient analytic
        operations on modern hardware.</p>

    <p>Feather uses the Apache Arrow columnar memory specification to represent binary
        data on disk. This makes read and write operations very fast. This is particularly
        important for encoding null/NA values and variable-length types like UTF8 strings.
        Feather is a part of the broader Apache Arrow project. Feather defines its own
        simplified schemas and metadata for on-disk representation.</p>

    <p>In the below example, we write a DataFrame into Feather
        file and then read it back.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> val temp = java.io.File.createTempFile("chinook", "arrow")
    temp: java.io.File = /var/folders/cb/577dvd4n2db0ghdn3gn7ss0h0000gn/T/chinook5413820941564790310arrow

    smile> val path = temp.toPath()
    path: java.nio.file.Path = /var/folders/cb/577dvd4n2db0ghdn3gn7ss0h0000gn/T/chinook5413820941564790310arrow

    smile> write.arrow(df, path)
    [main] INFO smile.io.Arrow - write 1000 rows

    smile> val df = read.arrow(path)
    df: DataFrame = [registration_dttm: DateTime, id: Integer, first_name: String, last_name: String, email: String, gender: String, ip_address: String, cc: String, country: String, birthdate: String, salary: Double, title: String, comments: String]
    +-------------------+---+----------+---------+--------------------+------+--------------+----------------+--------------------+----------+---------+--------------------+--------+
    |  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|             country| birthdate|   salary|               title|comments|
    +-------------------+---+----------+---------+--------------------+------+--------------+----------------+--------------------+----------+---------+--------------------+--------+
    |2016-02-03T07:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|   1E+02|
    |2016-02-03T17:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |              Canada| 1/16/1968|150280.17|       Accountant IV|        |
    |2016-02-03T01:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|        |
    |2016-02-03T00:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female| 140.35.109.83|3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|        |
    |2016-02-03T05:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      |169.113.235.40|5602256255204850|        South Africa|          |     null|                    |        |
    |2016-02-03T07:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female|195.131.81.179|3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|        |
    |2016-02-03T08:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male|232.234.81.197|3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|        |
    |2016-02-03T06:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|  91.235.51.73|                |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|        |
    ...
    </code></pre>

    <h3 id="read.sas">SAS7BDAT</h3>
    <p>SAS7BDAT is currently the main format
        used for storing SAS datasets across all platforms.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> val df = read.sas(Paths.getTestData("sas/airline.sas7bdat"))
    df: DataFrame = [YEAR: double, Y: double, W: double, R: double, L: double, K: double]
    +----+-----+-----+------+-----+-----+
    |YEAR|    Y|    W|     R|    L|    K|
    +----+-----+-----+------+-----+-----+
    |1948|1.214|0.243|0.1454|1.415|0.612|
    |1949|1.354| 0.26|0.2181|1.384|0.559|
    |1950|1.569|0.278|0.3157|1.388|0.573|
    |1951|1.948|0.297| 0.394| 1.55|0.564|
    |1952|2.265| 0.31|0.3559|1.802|0.574|
    |1953|2.731|0.322|0.3593|1.926|0.711|
    |1954|3.025|0.335|0.4025|1.964|0.776|
    |1955|3.562| 0.35|0.3961|2.116|0.827|
    |1956|3.979|0.361|0.3822|2.435|  0.8|
    |1957| 4.42|0.379|0.3045|2.707|0.921|
    +----+-----+-----+------+-----+-----+
    22 more rows...
    </code></pre>

    <h3 id="read.jdbc">Relational Database</h3>
    <p>It is also easy to load data from relation databases through JDBC.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> import $ivy.`org.xerial:sqlite-jdbc:3.28.0`
    import $ivy.$

    smile> Class.forName("org.sqlite.JDBC")
    res23: Class[?0] = class org.sqlite.JDBC

    smile> val url = String.format("jdbc:sqlite:%s", Paths.getTestData("sqlite/chinook.db").toAbsolutePath())
    url: String = "jdbc:sqlite:data/sqlite/chinook.db"
    smile> val sql = """select e.firstname as 'Employee First', e.lastname as 'Employee Last', c.firstname as 'Customer First', c.lastname as 'Customer Last', c.country, i.total
                     from employees as e
                     join customers as c on e.employeeid = c.supportrepid
                     join invoices as i on c.customerid = i.customerid
                    """
    sql: String = """select e.firstname as 'Employee First', e.lastname as 'Employee Last', c.firstname as 'Customer First', c.lastname as 'Customer Last', c.country, i.total
                     from employees as e
                     join customers as c on e.employeeid = c.supportrepid
                     join invoices as i on c.customerid = i.customerid
                    """

    smile> val conn = java.sql.DriverManager.getConnection(url)
    conn: java.sql.Connection = org.sqlite.jdbc4.JDBC4Connection@782cd00

    smile> val stmt = conn.createStatement()
    stmt: java.sql.Statement = org.sqlite.jdbc4.JDBC4Statement@40df1311

    smile> val rs = stmt.executeQuery(sql)
    rs: java.sql.ResultSet = org.sqlite.jdbc4.JDBC4ResultSet@5a524a19

    smile> val df = DataFrame.of(rs)
    df: DataFrame = [Employee First: String, Employee Last: String, Customer First: String, Customer Last: String, Country: String, Total: Double]
    +--------------+-------------+--------------+-------------+-------+-----+
    |Employee First|Employee Last|Customer First|Customer Last|Country|Total|
    +--------------+-------------+--------------+-------------+-------+-----+
    |          Jane|      Peacock|          Luís|    Gonçalves| Brazil| 3.98|
    |          Jane|      Peacock|          Luís|    Gonçalves| Brazil| 3.96|
    |          Jane|      Peacock|          Luís|    Gonçalves| Brazil| 5.94|
    |          Jane|      Peacock|          Luís|    Gonçalves| Brazil| 0.99|
    |          Jane|      Peacock|          Luís|    Gonçalves| Brazil| 1.98|
    |          Jane|      Peacock|          Luís|    Gonçalves| Brazil|13.86|
    |          Jane|      Peacock|          Luís|    Gonçalves| Brazil| 8.91|
    |         Steve|      Johnson|        Leonie|       Köhler|Germany| 1.98|
    |         Steve|      Johnson|        Leonie|       Köhler|Germany|13.86|
    |         Steve|      Johnson|        Leonie|       Köhler|Germany| 8.91|
    +--------------+-------------+--------------+-------------+-------+-----+
    402 more rows...
    </code></pre>

    <h3 id="read.arff">Weka ARFF</h3>
    <p><a href="http://www.cs.waikato.ac.nz/ml/weka/arff.html">Weka ARFF (attribute relation file format)</a>
        is an ASCII text file format that is essentially a CSV file with a header that describes the meta data.
        ARFF was developed for use in the <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a> machine learning software.</p>

    <p>A dataset is firstly described, beginning with the name of the dataset (or the relation in ARFF terminology).
        Each of the variables (or attribute in ARFF terminology) used to describe the observations is then identified,
        together with their data type, each definition on a single line. The actual observations are then listed,
        each on a single line, with fields separated by commas, much like a CSV file.</p>

    <p>Missing values in an ARFF dataset are identified using the question mark '?'.
        Comments can be included in the file, introduced at the beginning of a line with a '%',
        whereby the remainder of the line is ignored.</p>

    <p>A significant advantage of the ARFF data file over the CSV data file is the meta data information.
        Also, the ability to include comments ensure we can record extra information about the data set,
        including how it was derived, where it came from, and how it might be cited.</p>

    <p>To read an ARFF file, <code>smile.io</code> has the function</p>

    <pre class="prettyprint lang-scala"><code>
    def read.arff(file: String): DataFrame
    </code></pre>

    <p>In the directory <code>data/weka</code>,
        we have many sample ARFF files. We can also read data from remote servers
        by HTTP, FTP, etc.</p>

    <pre class="prettyprint lang-scala"><code>
    smile> val df = read.arff("https://github.com/haifengl/smile/blob/master/shell/src/universal/data/weka/cpu.arff?raw=true")
    [main] INFO smile.io.Arff - Read ARFF relation cpu
    df: DataFrame = [MYCT: float, MMIN: float, MMAX: float, CACH: float, CHMIN: float, CHMAX: float, class: float]
    +----+-----+-----+----+-----+-----+-----+
    |MYCT| MMIN| MMAX|CACH|CHMIN|CHMAX|class|
    +----+-----+-----+----+-----+-----+-----+
    | 125|  256| 6000| 256|   16|  128|  199|
    |  29| 8000|32000|  32|    8|   32|  253|
    |  29| 8000|32000|  32|    8|   32|  253|
    |  29| 8000|32000|  32|    8|   32|  253|
    |  29| 8000|16000|  32|    8|   16|  132|
    |  26| 8000|32000|  64|    8|   32|  290|
    |  23|16000|32000|  64|   16|   32|  381|
    |  23|16000|32000|  64|   16|   32|  381|
    |  23|16000|64000|  64|   16|   32|  749|
    |  23|32000|64000| 128|   32|   64| 1238|
    +----+-----+-----+----+-----+-----+-----+
    199 more rows...
    </code></pre>

    <h3 id="read.csv">Delimited Text and CSV</h3>
    <p>The delimited text files are widely used in machine learning research community.
        The comma-separated values (CSV) file is a special case. Smile provides flexible
        parser for them.</p>

    <pre class="prettyprint lang-scala"><code>
    def csv(file: String, delimiter: Char = ',', header: Boolean = true, quote: Char = '"', escape: Char = '\\', schema: StructType = null): DataFrame
    </code></pre>
    <p>The parser can infer the schema of data.</p>

    <pre class="prettyprint lang-scala"><code>
    val zip = read.csv("data/usps/zip.train", delimiter = ' ', header = false)
    </code></pre>

    <h3 id="read.libsvm">LibSVM</h3>
    <p>LibSVM is a very fast and popular library for support vector machines.
        LibSVM uses a sparse format where zero values do not need to be stored.
        Each line of a libsvm file is in the format:</p>
    <pre><code>
    &lt;label&gt; &lt;index1&gt;:&lt;value1&gt; &lt;index2&gt;:&lt;value2&gt; ...
    </code></pre>
    <p>where &lt;label&gt; is the target value of the training data.
        For classification, it should be an integer which identifies a class
        (multi-class classification is supported). For regression, it's any real
        number. For one-class SVM, it's not used so can be any number.
        &lt;index&gt; is an integer starting from 1, and &lt;value&gt;
        is a real number. The indices must be in an ascending order. The labels in
        the testing data file are only used to calculate accuracy or error. If they
        are unknown, just fill this column with a number.</p>

    <p>To read a libsvm file, <code>smile.io</code> has the function</p>

    <pre class="prettyprint lang-scala"><code>
    def read.libsvm(file: String): Dataset[Instance[SparseArray]]
    </code></pre>

    <p>Although libsvm employs a sparse format, most libsvm files contain dense data.
        Therefore, Smile also provides helper functions to convert
        it to dense arrays.</p>

    <pre class="prettyprint lang-scala"><code>
    val glass = read.libsvm("data/libsvm/glass.txt")
    </code></pre>

    <p>In case of truly sparse libsvm data, we can convert it to <code>SparseMatrix</code>
        for more efficient matrix computation.</p>
    <pre class="prettyprint lang-scala"><code>
    smile&gt; val sparse = glass.toSparseMatrix
    sparse: smile.math.matrix.SparseMatrix = smile.math.matrix.SparseMatrix@73ad7e90
    </code></pre>

    <h3 id="sparse-format">Coordinate Triple Tuple List</h3>

    <p>The function <code>SparseDataset.from(Path path, int arrayIndexOrigin)</code>
        can read sparse data in coordinate triple tuple list format. The parameter
        <code>arrayIndexOrigin</code> is the starting index of array. By default, it is
        0 as in C/C++ and Java. But it could be 1 to parse data produced
        by other programming language such as Fortran.</p>

    <p>The coordinate file stores a list of (row, column, value) tuples:</p>
    <pre>
    instanceID attributeID value
    instanceID attributeID value
    instanceID attributeID value
    instanceID attributeID value
    ...
    instanceID attributeID value
    instanceID attributeID value
    instanceID attributeID value
    </pre>

    <p>Ideally, the entries are sorted (by row index, then column index) to improve
        random access times. This format is good for incremental matrix
        construction.</p>

    <p>Optionally, there may be 2 header lines</p>

    <pre>
    D    // The number of instances
    W    // The number of attributes
    </pre>

    <p>or 3 header lines</p>

    <pre>
    D    // The number of instances
    W    // The number of attributes
    N    // The total number of nonzero items in the dataset.
    </pre>
    <p>These header lines will be ignored.</p>

    <p>The sample data <code>data/text/kos.txt</code> is in the coordinate format.</p>

    <pre class="prettyprint lang-scala"><code>
    val kos = SparseDataset.from(java.nio.file.Paths.get("data/text/kos.txt"), 1)
    </code></pre>

    <h3 id="Harwell-Boeing">Harwell-Boeing Column-Compressed Sparse Matrix</h3>

    <p>In Harwell-Boeing column-compressed sparse matrix file, nonzero values are stored in an array
        (top-to-bottom, then left-to-right-bottom). The row indices corresponding to
        the values are also stored. Besides, a list of pointers are indexes where
        each column starts. The class SparseMatrix supports two formats for Harwell-Boeing files.
        The simple one is organized as follows:</p>

    <p>The first line contains three integers, which are the number of rows,
        the number of columns, and the number of nonzero entries in the matrix.</p>

    <p>Following the first line, there are m + 1 integers that are the indices of
        columns, where m is the number of columns. Then there are n integers that
        are the row indices of nonzero entries, where n is the number of nonzero
        entries. Finally, there are n float numbers that are the values of nonzero
        entries.</p>

    <p>The function <code>SparseMatrix.text(Path path)</code> can read this simple
        format. In the directory <code>data/matrix</code>, there are several sample files in
        the Harwell-Boeing format.</p>

    <pre class="prettyprint lang-scala"><code>
    val blocks = SparseMatrix.text("data/matrix/08blocks.txt")
    </code></pre>

    <p>The second format is more complicated and powerful, called Harwell-Boeing Exchange Format.
        For details, see <a href="http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html">http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html</a>.
        Note that our implementation supports only real-valued matrix and we ignore
        the optional right hand side vectors. This format is supported by the function
        <code>SparseMatrix.harwell(Path path)</code>. </p>

    <h3 id="wireframe">Wireframe</h3>
    <p>Smile can parse 3D wireframe models in Wavefront OBJ files.</p>

    <pre class="prettyprint lang-scala"><code>
    def read.wavefront(file: String): (Array[Array[Double]], Array[Array[Int]])
    </code></pre>

    <p>In the directory <code>data/wireframe</code>, there is a teapot wireframe model. In the
        next section, we will learn how to visualize the 3D wireframe models.</p>

    <pre class="prettyprint lang-scala"><code>
    val (vertices, edges) = read.wavefront("data/wireframe/teapot.obj")
    </code></pre>

    <h2 id="export">Export Data and Models</h2>

    <p>To serialize a model, you may use</p>
    <pre class="prettyprint lang-scala"><code>
    write(model, file)
    </code></pre>
    <p>This method is in the Scala API <code>smile.write</code> object and serialize the model to Java
        serialization format. This is handy if you want to use a model in Spark.</p>

    <p>Alternatively, you can also use</p>
    <pre class="prettyprint lang-scala"><code>
    write.xstream(model, file)
    </code></pre>

    <p>which uses XStream library to serialize the model (actually any objects) to XML file.</p>

    <p>To read the model back, you can use <code>read(file)</code> or <code>read.xstream(file)</code>, correspondingly.</p>

    <p>You can also save a <code>DataFrame</code> to an ARFF file with the method
        <code>write.arff(data, file)</code>. The ARFF file keeps the data type information.
        If you prefer the plain csv text file, you may use the methods <code>write.csv(data, file)</code> or
        <code>write.table(data, file, "delimiter")</code>, which save a generic two dimensional array
        with comma or customized delimiter. To save one dimensional array, simply call
        <code>write(array, file)</code>.</p>

    <div id="btnv">
        <span class="btn-arrow-left">&larr; &nbsp;</span>
        <a class="btn-prev-text" href="overview.html" title="Previous Section: What's Machine Learning"><span>What's Machine Learning</span></a>
        <a class="btn-next-text" href="visualization.html" title="Next Section: Classification"><span>Visualization</span></a>
        <span class="btn-arrow-right">&nbsp;&rarr;</span>
    </div>
</div>

<script type="text/javascript">
    $('#toc').toc({exclude: 'h1, h5, h6', context: '', autoId: true, numerate: false});
</script>
